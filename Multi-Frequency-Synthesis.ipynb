{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MFS\n",
    "______________________________________________________________________________\n",
    "\n",
    "Algorithm: Rau and Cornwell MFS\n",
    "\n",
    "Implementation (C) L Bester, SKA-SA\n",
    "\n",
    "Credit: B Hugo's gridding implementation, SKA-SA\n",
    "    \n",
    "Warning: Storing the DFT kernels makes this a memory hungry beast. \n",
    "With 64 pixels and 16 channels a run requires about 6GB \n",
    "With 128 pixels and 16 channels a run requires about 13GB\n",
    "______________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math as mp\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyrap.tables import table as tbl\n",
    "import scipy.signal\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from IPython.html.widgets import FloatProgress\n",
    "from IPython.display import display\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "from scipy import optimize as opt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AA_filter:\n",
    "    \"\"\"\n",
    "    Anti-Aliasing filter\n",
    "    \n",
    "    Keyword arguments for __init__:\n",
    "    filter_half_support --- Half support (N) of the filter; the filter has a full support of N*2 + 1 taps\n",
    "    filter_oversampling_factor --- Number of spaces in-between grid-steps (improves gridding/degridding accuracy)\n",
    "    filter_type --- box (nearest-neighbour), sinc or gaussian_sinc\n",
    "    \"\"\"\n",
    "    half_sup = 0\n",
    "    oversample = 0\n",
    "    full_sup_wo_padding = 0\n",
    "    full_sup = 0\n",
    "    no_taps = 0\n",
    "    filter_taps = None\n",
    "    def __init__(self, filter_half_support, filter_oversampling_factor, filter_type):\n",
    "        self.half_sup = filter_half_support\n",
    "        self.oversample = filter_oversampling_factor\n",
    "        self.full_sup_wo_padding = (filter_half_support * 2 + 1)\n",
    "        self.full_sup = self.full_sup_wo_padding + 2 #+ padding\n",
    "        self.no_taps = self.full_sup + (self.full_sup - 1) * (filter_oversampling_factor - 1)\n",
    "        taps = np.arange(self.no_taps)/float(filter_oversampling_factor) - self.full_sup / 2\n",
    "        if filter_type == \"box\":\n",
    "            self.filter_taps = np.where((taps >= -0.5) & (taps <= 0.5),\n",
    "                                        np.ones([len(taps)]),np.zeros([len(taps)]))\n",
    "        elif filter_type == \"sinc\":\n",
    "            self.filter_taps = np.sinc(taps)\n",
    "        elif filter_type == \"gaussian_sinc\":\n",
    "            alpha_1=1.55\n",
    "            alpha_2=2.52\n",
    "            self.filter_taps = np.sin(np.pi/alpha_1*(taps+0.00000000001))/(np.pi*(taps+0.00000000001))*np.exp(-(taps/alpha_2)**2)\n",
    "        else:\n",
    "            raise ValueError(\"Expected one of 'box','sinc' or 'gausian_sinc'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class grid_it(object):\n",
    "    \n",
    "    def __init__(self,uvw,ref_lda,Nx,Ny,convolution_filter,degrid_mode=\"normal\"):\n",
    "        self.uvw = uvw\n",
    "        self.ref_lda = ref_lda\n",
    "        self.Nx = Nx\n",
    "        self.Ny = Ny\n",
    "        self.nchan = ref_lda.size\n",
    "        self.nvis = uvw.shape[0]\n",
    "        self.convolution_filter = convolution_filter\n",
    "    \n",
    "    def Init_DFT(self,ra0,dec0,uvw,delta_pix):\n",
    "        #Set coords (should set delta_pix in x and y directions if not square image)\n",
    "        ra = ra0 + np.linspace(ra0 - self.Nx*delta_pix/2.0,ra0 + self.Nx*delta_pix/2.0,npix)\n",
    "        delta_ra = ra - ra0\n",
    "        dec = dec0 + np.linspace(dec0 - self.Ny*delta_pix/2.0,dec0 + self.Ny*delta_pix/2.0,npix)\n",
    "        l = (np.cos(dec)*np.sin(delta_ra))\n",
    "        m = (-np.sin(dec)*np.cos(dec0) - np.cos(dec)*np.sin(dec0)*cos(delta_ra))\n",
    "        #Get l,m,n triple\n",
    "        self.ncoord = np.zeros(self.Nx*self.Ny)\n",
    "        lmn = np.zeros([self.Nx*self.Ny,3])\n",
    "        for i in xrange(self.Nx):\n",
    "            for j in xrange(self.Ny):\n",
    "                self.ncoord[i*self.Ny + j] = np.sqrt(1.0 - l[i]**2 - m[j]**2)\n",
    "                lmn[i*self.Ny + j,0] = l[i]\n",
    "                lmn[i*self.Ny + j,1] = m[j]\n",
    "                lmn[i*self.Ny + j,2] = self.ncoord[i*self.Ny+j] - 1.0        \n",
    "        #Precompute DFT kernel\n",
    "        tmp = -2.0j*np.pi*dot(uvw,lmn.T)\n",
    "        self.K = np.zeros([self.nchan,self.nvis,self.Nx*self.Ny],dtype=complex)\n",
    "        for k in range(self.nchan):\n",
    "                self.K[k,:,:] = np.exp(tmp/self.ref_lda[k])    \n",
    "        \n",
    "    def do_DFT_Vpred(self,IM):\n",
    "        \"\"\"\n",
    "        DFT to predict Vis from IM\n",
    "        \"\"\"\n",
    "        tmp = np.zeros([self.nchan,self.nvis],dtype=complex)\n",
    "        for i in range(self.K.shape[0]):\n",
    "            tmp[i,:] = np.dot(self.K[i],np.flipud(IM[i]).T.flatten()/self.ncoord)\n",
    "        return tmp.T #/2\n",
    "            \n",
    "    def give_IR(self,vis):\n",
    "        filter_index = np.arange(-self.convolution_filter.half_sup,self.convolution_filter.half_sup+1)\n",
    "        measurement_regular = np.zeros([self.nchan,self.Ny,self.Nx],dtype=np.complex) #one grid for the resampled visibilities\n",
    "\n",
    "        for r in range(0,self.nvis):\n",
    "            for c in range(self.nchan):\n",
    "                scaled_uv = self.uvw[r,:] / self.ref_lda[c] \n",
    "                disc_u = int(scaled_uv[0])\n",
    "                disc_v = int(scaled_uv[1])\n",
    "                frac_u_offset = int((self.convolution_filter.half_sup + 1 + (-scaled_uv[0] + disc_u)) * self.convolution_filter.oversample)\n",
    "                frac_v_offset = int((self.convolution_filter.half_sup + 1 + (-scaled_uv[1] + disc_v)) * self.convolution_filter.oversample)\n",
    "                if (disc_v + self.Ny // 2 + self.convolution_filter.half_sup >= self.Ny or \n",
    "                    disc_u + self.Nx // 2 + self.convolution_filter.half_sup >= self.Nx or\n",
    "                    disc_v + self.Ny // 2 - self.convolution_filter.half_sup < 0 or \n",
    "                    disc_u + self.Nx // 2 - self.convolution_filter.half_sup < 0): \n",
    "                    continue\n",
    "                for conv_v in filter_index:\n",
    "                    v_tap = self.convolution_filter.filter_taps[conv_v * self.convolution_filter.oversample + frac_v_offset]  \n",
    "                    grid_pos_v = disc_v + conv_v + self.Ny // 2\n",
    "                    for conv_u in filter_index:\n",
    "                        u_tap = self.convolution_filter.filter_taps[conv_u * self.convolution_filter.oversample + frac_u_offset]\n",
    "                        conv_weight = v_tap * u_tap\n",
    "                        grid_pos_u = disc_u + conv_u + self.Nx // 2\n",
    "                        measurement_regular[c,grid_pos_v,grid_pos_u] += vis[r,c] * conv_weight              \n",
    "        dirty = np.zeros(measurement_regular.shape,dtype=measurement_regular.dtype)\n",
    "        for c in range(self.nchan):\n",
    "            dirty[c,:,:] = np.fft.fftshift(np.fft.ifft2(np.fft.ifftshift(measurement_regular[c,:,:])))\n",
    "        return dirty/4.0\n",
    "    \n",
    "    def give_Vpred(self,Image):\n",
    "        \"\"\"\n",
    "        Predict with convolutional degridder (not working at the moment)\n",
    "        \"\"\"\n",
    "        filter_index = np.arange(-self.convolution_filter.half_sup,self.convolution_filter.half_sup+1)\n",
    "        \n",
    "        vis_grid = np.zeros([self.nchan,self.Nx,self.Ny])\n",
    "        for i in range(self.nchan):\n",
    "            vis_grid[i] = fft.fftshift(fft.fft2(fft.ifftshift(Image[i])))\n",
    "        vis = np.zeros([self.uvw.shape[0],self.ref_lda.size],dtype=complex)\n",
    "        for r in range(0,self.nvis):\n",
    "            for c in range(self.nchan):\n",
    "                scaled_uv = self.uvw[r,:] / self.ref_lda[c] \n",
    "                disc_u = int(scaled_uv[0])\n",
    "                disc_v = int(scaled_uv[1])\n",
    "                frac_u_offset = int((self.convolution_filter.half_sup + 1 + (-scaled_uv[0] + disc_u)) * self.convolution_filter.oversample)\n",
    "                frac_v_offset = int((self.convolution_filter.half_sup + 1 + (-scaled_uv[1] + disc_v)) * self.convolution_filter.oversample)\n",
    "                if (disc_v + self.Ny // 2 + self.convolution_filter.half_sup >= self.Ny or \n",
    "                    disc_u + self.Nx // 2 + self.convolution_filter.half_sup >= self.Nx or\n",
    "                    disc_v + self.Ny // 2 - self.convolution_filter.half_sup < 0 or \n",
    "                    disc_u + self.Nx // 2 - self.convolution_filter.half_sup < 0): \n",
    "                    continue\n",
    "                for conv_v in filter_index:\n",
    "                    v_tap = self.convolution_filter.filter_taps[conv_v * self.convolution_filter.oversample + frac_v_offset]  \n",
    "                    grid_pos_v = disc_v + conv_v + self.Ny // 2\n",
    "                    for conv_u in filter_index:\n",
    "                        u_tap = self.convolution_filter.filter_taps[conv_u * self.convolution_filter.oversample + frac_u_offset]\n",
    "                        conv_weight = v_tap * u_tap\n",
    "                        grid_pos_u = disc_u + conv_u + self.Nx // 2\n",
    "                        vis[r,c] += vis_grid[c,grid_pos_v,grid_pos_u] * conv_weight              \n",
    "        return vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_ifft(vis,uvw,ref_lda,Nx,Ny,convolution_filter):\n",
    "    \"\"\"\n",
    "    Convolutional gridder (continnuum)\n",
    "    \n",
    "    Keyword arguments:\n",
    "    vis --- Visibilities as sampled by the interferometer\n",
    "    uvw --- interferometer's scaled uvw coordinates. (Prerequisite: these uv points are already scaled by the simularity\n",
    "            theorem, such that -N_x*Cell_l*0.5 <= theta_l <= N_x*Cell_l*0.5 and\n",
    "            -N_y*Cell_m*0.5 <= theta_m <= N_y*Cell_m*0.5\n",
    "    ref_lda --- array of reference lambdas (size of vis channels)\n",
    "    Nx,Ny --- size of image in pixels\n",
    "    convolution_filter --- pre-instantiated AA_filter anti-aliasing filter object\n",
    "    \"\"\"\n",
    "    assert vis.shape[1] == ref_lda.shape[0], (vis.shape[1], ref_lda.shape[0])\n",
    "    filter_index = np.arange(-convolution_filter.half_sup,convolution_filter.half_sup+1)\n",
    "    measurement_regular = np.zeros([vis.shape[1],vis.shape[2],Ny,Nx],dtype=np.complex) #one grid for the resampled visibilities\n",
    "    #for deconvolution the PSF should be 2x size of the image (see Hogbom CLEAN for details):\n",
    "    sampling_regular = np.zeros([vis.shape[1],2*Ny,2*Nx],dtype=np.complex) #one grid for the resampled sampling function\n",
    "    \n",
    "    pbar = FloatProgress(min=0, max=100)\n",
    "    display(pbar)\n",
    "    for r in range(0,uvw.shape[0]):\n",
    "        pbar.value = r / float(uvw.shape[0]) * 100\n",
    "        for c in range(vis.shape[1]):\n",
    "            scaled_uv = uvw[r,:] / ref_lda[c] \n",
    "            disc_u = int(scaled_uv[0])\n",
    "            disc_v = int(scaled_uv[1])\n",
    "            frac_u_offset = int((convolution_filter.half_sup + 1 + (-scaled_uv[0] + disc_u)) * convolution_filter.oversample)\n",
    "            frac_v_offset = int((convolution_filter.half_sup + 1 + (-scaled_uv[1] + disc_v)) * convolution_filter.oversample)\n",
    "            disc_u_psf = int(scaled_uv[0]*2)\n",
    "            disc_v_psf = int(scaled_uv[1]*2)\n",
    "            frac_u_offset_psf = int((convolution_filter.half_sup + 1 + (-scaled_uv[0]*2 + disc_u_psf)) * convolution_filter.oversample)\n",
    "            frac_v_offset_psf = int((convolution_filter.half_sup + 1 + (-scaled_uv[1]*2 + disc_v_psf)) * convolution_filter.oversample)\n",
    "            if (disc_v + Ny // 2 + convolution_filter.half_sup >= Ny or \n",
    "                disc_u + Nx // 2 + convolution_filter.half_sup >= Nx or\n",
    "                disc_v + Ny // 2 - convolution_filter.half_sup < 0 or \n",
    "                disc_u + Nx // 2 - convolution_filter.half_sup < 0): \n",
    "                continue\n",
    "            for conv_v in filter_index:\n",
    "                v_tap = convolution_filter.filter_taps[conv_v * convolution_filter.oversample + frac_v_offset]  \n",
    "                v_tap_psf = convolution_filter.filter_taps[conv_v * convolution_filter.oversample + frac_v_offset_psf]  \n",
    "                grid_pos_v = disc_v + conv_v + Ny // 2\n",
    "                grid_pos_v_psf = disc_v_psf + conv_v + Ny\n",
    "                for conv_u in filter_index:\n",
    "                    u_tap = convolution_filter.filter_taps[conv_u * convolution_filter.oversample + frac_u_offset]\n",
    "                    u_tap_psf = convolution_filter.filter_taps[conv_u * convolution_filter.oversample + frac_u_offset_psf]\n",
    "                    conv_weight = v_tap * u_tap\n",
    "                    conv_weight_psf = v_tap_psf * u_tap_psf\n",
    "                    grid_pos_u = disc_u + conv_u + Ny // 2\n",
    "                    grid_pos_u_psf = disc_u_psf + conv_u + Nx\n",
    "                    for p in range(vis.shape[2]):\n",
    "                        measurement_regular[c,p,grid_pos_v,grid_pos_u] += vis[r,c,p] * conv_weight\n",
    "                    sampling_regular[c,grid_pos_v_psf,grid_pos_u_psf] += (1+0.0j) * conv_weight_psf\n",
    "                    \n",
    "    dirty = np.zeros(measurement_regular.shape,dtype=measurement_regular.dtype)\n",
    "    psf = np.zeros(sampling_regular.shape,dtype=sampling_regular.dtype)\n",
    "    \n",
    "    for c in range(vis.shape[1]):\n",
    "        for p in range(vis.shape[2]):\n",
    "            dirty[c,p,:,:] = np.fft.fftshift(np.fft.ifft2(np.fft.ifftshift(measurement_regular[c,p,:,:])))\n",
    "        psf[c,:,:] = np.fft.fftshift(np.fft.ifft2(np.fft.ifftshift(sampling_regular[c,:,:])))\n",
    "    return dirty/4.0,psf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative $\\chi^2$ minimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imaging problem can be formulated as an iterative $\\chi^2$ minimisation of a system of linear equations of the form\n",
    "$$ Ax = b + \\epsilon,$$\n",
    "where $\\epsilon$ is iid Gaussian noise. This allows us to form the $\\chi^2$ distribution as\n",
    "$$ \\chi^2 = (Ax - b)^\\dagger W (Ax - b), $$\n",
    "where $W$ is a diagonal matrix of weights and the superscript $\\dagger$ denotes the conjugate transpose operator. The assumption that $\\epsilon$ is Gaussian noise basically ensures that the quantity we are optimising only has a single stationary point. If $\\epsilon$ was not Gaussian but rather some multi-modal distribution, then the quantity $\\epsilon^\\dagger \\epsilon$ could also have a multi-modal distribution. \n",
    "\n",
    "The goal is to find the set of parameters $x$ which minimises $\\chi^2$. The gradient of $\\chi^2$ is given by\n",
    "$$ \\partial_x \\chi^2 = J(x) = A^\\dagger W (b - Ax), $$\n",
    "where we have used the standard linear algebra notation $J(x)$ to denote the Jacobian (or, since $\\chi^2$ is scalar, also the gradient). One more derivative gives the Hessian $H(x)$ as \n",
    "$$ \\partial^2_x \\chi^2 = H = A^\\dagger W A. $$\n",
    "Furthermore, setting the gradient to zero produces the normal equations\n",
    "$$ A^\\dagger W A x = A^\\dagger W b, $$\n",
    "If the Hessian matrix $H = A^H W A$ is non-singular we could solve for $x$ directly from the normal equations\n",
    "$$ x = \\left( A^\\dagger W A  \\right)^{-1} A^\\dagger W b. $$\n",
    "Unfortunately, because of incomplete $uv$-coverage, in interferometry the Hessian matrix will almost always be singular. In this case an approximate iterative solution can still be found (from an initial guess, $x_0$ say) using a numerical root finding algorithm such as Newton's method i.e.\n",
    "$$ x_{i+1} = x_i + \\tilde{H}^{-1}(x_i) J(x_i), $$\n",
    "where $\\tilde{H}(\\cdot)$ is some invertible approximation of the Hessian matrix (eg. a diagonal approximation). When the Hessian does not have full column rank the solution to the normal equations is not unique. The solution is ambiguous w.r.t. the addition of any vector $\\tilde{x}$ in the null space of the Hessian i.e. $H\\tilde{x} = 0$. The reason for this is that, according to the normal equations, we have\n",
    "$$ H(x + \\tilde{x}) = Hx = A^\\dagger W b, $$\n",
    "regardless of the choice of $\\tilde{x}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imaging as an iterative $\\chi^2$ minimisation problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imaging problem can be formulated according to the above framework by identifying $A = SF$ i.e.\n",
    "$$ S F I = V_{obs} + \\epsilon,$$\n",
    "where $S$ is the sampling operator, $F$ the Fourier transform and $I$ is the image we want to deconvolve. Direct substitution yields the $\\chi^2$ distribution as\n",
    "$$ \\chi^2 = (S F I - V_{obs})^\\dagger W (S F I - V_{obs}), $$\n",
    "where $W$ are the imaging weights. The goal is to find the image $I$ which minimises $\\chi^2$. \n",
    "In this case the gradient is given by\n",
    "$$ \\partial_x \\chi^2 = J(I) = F^\\dagger S^\\dagger W (S F I - V_{obs}), $$\n",
    "and the Hessian is \n",
    "$$ \\partial^2_x \\chi^2 = H = F^\\dagger S^\\dagger W S F. $$\n",
    "Since $S$ will not, in general, have full column rank we have to find the solution iteratively using an update rule such as\n",
    "$$ I_{i+1} = I_i + g \\tilde{H}^{-1} J(I_i), $$\n",
    "where $g$ controls the step size. The normal equations are given by\n",
    "$$ HI = I_D, \\quad \\mbox{where} \\quad H = F^\\dagger S^\\dagger W SF, \\quad \\mbox{and} \\quad I^D = F^\\dagger S^\\dagger W V_{obs}.  $$\n",
    "Here $I_D$ denotes the dirty image which can be thought of as the image $I$ convolved with the point spread function, denoted $I^{PSF}$, of the instrument i.e. $I^D = I^{PSF} * I$. For this reason the solution to the normal equations can be thought of as a deconvolution. \n",
    "\n",
    "In imaging the iterative solution is usually implemented using major and minor cycles. To see how this works first note that, for a given model image $I^{M}$, the Jacobian is just the residual image i.e. \n",
    "$$ J(I^{M}) = F^\\dagger S^\\dagger W (S F I^M - V_{obs}) = I^{R}. $$\n",
    "Thus one iteration of the optimisation method would be given by \n",
    "$$ I^M_{i+1} = I^M_{i} + g \\tilde{H}^{-1} I^R_{i}, $$\n",
    "where $\\tilde{H}$ is constructed by invoking a number of a priori assumptions. Recall that an ideal interferometer (i.e. one with complete $uv$-coverage) will have a delta function as its PSF. This suggests that we can, in principle, eliminate the corrupting effects of the instrument by assuming that the PSF is a delta function. This is equivalent to a diagonal approximation of the Hessian matrix. If we also assume that the PSF is spatially invariant all the elements on the diagonal will be the same and the Hessian reduces to a single number viz. the value of $I^{PSF}$ at the center. With these two assumptions we can construct \n",
    "$$ \\tilde{H} = \\mbox{mid}(I^{PSF}). $$\n",
    "This allows us to construct what is known as the principal solution $\\hat{I}^D$ as\n",
    "$$ \\hat{I}^D = \\tilde{H}^{-1} I^D, \\qquad \\hat{I}^{PSF} = \\tilde{H}^{-1} I^{PSF}. $$\n",
    "Note that we are using a hat to denote normalisation by the peak of the PSF. Thus the principal solution is basically just the dirty image normalised so that it has the same units as the image of the sky. Note that the inverse of the Hessian matrix will only exist (i.e. be finite) at locations corresponding to non-zero pixels in the image. Thus, to apply the update step, we start by searching for the peak in $I^D$ and only perform the Hessian inversion locally. We can then compute the new residual image (i.e. evaluate the Jacobian) and repeat until some prespecified convergence criterion has been reached. However, since evaluating the Jacobian is an expensive operation, the algorithm is usually implemented slightly differently. This is where the distinction between minor and major cycles comes in. Major cycles evaluate the Jacobian while minor cycles implement more efficient (but less accurate) updates to the model image. \n",
    "\n",
    "It is impractical to build the model image one pixel at a time. To speed things up a bit we might want to add multiple pixels (the brightest ones) to the model image during the minor cycle. This requires performing the subtraction (equivalently computing the Jacobian or updating the residual image $I^R$) in the image domain and can be implemented approximately by subtracting out the PSF centered at the location of the current brightest pixel from the entire image. However, since we are evaluating the PSF on a finite grid, this inevitably introduces discretisation errors which must be corrected for in the major cycle. Below we load some simulated data and illustrate the Cotton-Schwabb CLEAN algorithm (actually it is a slight modification which searches for the peak in the average image over channels which has the effect of increasing the SNR in the presence of noise). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load in MS\n",
    "ms = tbl(\"DATA/mfs_sky.MS_p0/\")\n",
    "\n",
    "#Get freq info\n",
    "msfreq = tbl(\"DATA/mfs_sky.MS_p0::SPECTRAL_WINDOW\")\n",
    "\n",
    "#Get pointing center (to compute DFT kernel)\n",
    "msfield = tbl(\"DATA/mfs_sky.MS_p0::FIELD\")\n",
    "ra0, dec0 = msfield.getcol(\"PHASE_DIR\").squeeze() #in radians\n",
    "\n",
    "#Set AA filter\n",
    "aa = AA_filter(3,63,\"sinc\")\n",
    "\n",
    "#Some constants and conversion factors\n",
    "c = 2.99792458e8 #speed of light\n",
    "npix = 65 #note using odd number otherwise centre of PSF is not well defined\n",
    "ARCSEC2RAD = 4.8481e-6\n",
    "delta_pix = 2 * ARCSEC2RAD * 2 * 2 * 2 * 2 #add or remove a power of two depending on image size\n",
    "uv_scale = npix * delta_pix\n",
    "\n",
    "#Select subset of data\n",
    "chanstep = 4\n",
    "nrows = 2000 #-1 to select all\n",
    "uvw = ms.getcol(\"UVW\")[0:nrows,:] \n",
    "vis = ms.getcol(\"DATA\")[0:nrows,0::chanstep,:]\n",
    "scaled_uvw = uvw * uv_scale\n",
    "\n",
    "#Get frequencies\n",
    "Freqs = msfreq.getcol(\"CHAN_FREQ\").squeeze()[0::chanstep]\n",
    "nchan = Freqs.size\n",
    "ref_freq = Freqs[nchan/2]\n",
    "\n",
    "#Close tables\n",
    "ms.close()\n",
    "msfreq.close()\n",
    "msfield.close()\n",
    "\n",
    "#Get wavelengths\n",
    "ref_lambda = c / Freqs\n",
    "\n",
    "#Compute dirty and psf using gridder\n",
    "print \"Getting PSF and ID\"\n",
    "dirty,psf = grid_ifft(vis,scaled_uvw,ref_lambda,npix,npix,aa)\n",
    "\n",
    "#Get ID and PSF\n",
    "ID = np.real((dirty[:,0,:,:]+dirty[:,3,:,:])*0.5)\n",
    "PSF = np.abs(psf)\n",
    "\n",
    "#Get V corresponding to stokes I \n",
    "Vobs = (vis[:,:,0] + vis[:,:,3])*0.5\n",
    "\n",
    "#Instantiate the gridder\n",
    "print \"Initialising grid machine\"\n",
    "gridder = grid_it(scaled_uvw,ref_lambda,npix,npix,aa)\n",
    "gridder.Init_DFT(ra0,dec0,uvw,delta_pix)\n",
    "\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cotton-Schwabb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def CLEAN_CS(Vobs,ID,PSF,gridder,\n",
    "             gamma = 0.05,threshold = \"Default\", nminoriter = 50,\n",
    "             nmajoriter = 50):\n",
    "    \"\"\"\n",
    "    This is a basic CLEAN implemented using major and minor cycles. \n",
    "    Input:  Vobs = precalibrated visibilities\n",
    "            ID = the un-nprmalised dirty image\n",
    "            PSF = the unnormalised PSF\n",
    "            gridder = instance of a gridder and degridder\n",
    "    \"\"\"\n",
    "    #Initialise model image\n",
    "    n = Vobs.shape[0]\n",
    "    nchan = Vobs.shape[1]\n",
    "    npix = ID.shape[-1]\n",
    "    IM = np.zeros([nchan,npix,npix])\n",
    "    \n",
    "    #Get maximum of PSF in all the channels\n",
    "    PSF_max = np.zeros(nchan)\n",
    "    for nu in xrange(nchan):\n",
    "        PSF_max[nu] = np.max(PSF[nu])\n",
    "        \n",
    "    #Get the mean image\n",
    "    Imean = np.mean(ID,axis=0)\n",
    "    \n",
    "    #Find first maximum\n",
    "    p,q = (argwhere(Imean == Imean.max())[0]).squeeze()\n",
    "    Istarm = Imean[p,q]\n",
    "    \n",
    "    #Get principle solution\n",
    "    Istar = ID[:,p,q]/PSF_max\n",
    "    \n",
    "    #print \"First peak = \", Istar\n",
    "    \n",
    "    #Set threshold\n",
    "    if threshold==\"Default\":\n",
    "        threshold = 0.25*np.abs(Istarm)\n",
    "        print \"Threshold set at \", threshold\n",
    "    else:\n",
    "        print \"Assuming user set threshold\"\n",
    "        \n",
    "    #Start deconvolution\n",
    "    i = 0\n",
    "    IR = ID.copy()\n",
    "    while (i < nmajoriter) and (Istarm > 0.25*threshold):\n",
    "        #Enter minor cycle\n",
    "        j = 0\n",
    "        while (j < nminoriter) and (Istarm > threshold):\n",
    "            #Update model image\n",
    "            IM[:,p,q] += gamma*Istar\n",
    "            \n",
    "            #Do image plane subtraction\n",
    "            IR -= gamma*Istar[:,np.newaxis,np.newaxis]*PSF[:,npix - p:2*npix - p,npix - q:2*npix - q] \n",
    "            \n",
    "            #Get mean image\n",
    "            Imean = np.mean(IR,axis=0)\n",
    "            \n",
    "            #Get new indices where IR is max\n",
    "            p,q = (argwhere(Imean == Imean.max())[0]).squeeze()\n",
    "            Istarm = Imean[p,q]\n",
    "            \n",
    "            #Get principal solution\n",
    "            Istar = IR[:,p,q]/PSF_max\n",
    "            \n",
    "            #Increment minor cycle counter\n",
    "            j += 1 \n",
    "        #Increment major cycle counter\n",
    "        i += 1\n",
    "        \n",
    "        #Update model image\n",
    "        IM[:,p,q] += 10*gamma*Istar\n",
    "        \n",
    "        #Get residual image\n",
    "        IR = get_Jacobian(Vobs,IM,gridder)\n",
    "        \n",
    "        #Get mean image\n",
    "        Imean = np.mean(IR,axis=0)        \n",
    "        \n",
    "        #Find new max\n",
    "        p,q = (argwhere(Imean == Imean.max())[0]).squeeze()\n",
    "        Istarm = Imean[p,q]\n",
    "        \n",
    "        #Get principle solution\n",
    "        Istar = IR[:,p,q]/PSF_max\n",
    "        \n",
    "        if i%5==0:\n",
    "            print \"i = \", i, \"j = \", j, \"Istarm = \", Istarm\n",
    "            \n",
    "    #Warn if number of iterations exceeded\n",
    "    if i >= nmajoriter:\n",
    "        print \"Max iterations exceeded. Istar = \",Istarm\n",
    "    print \"Done\"\n",
    "    return IM, IR\n",
    "    \n",
    "    \n",
    "def get_Jacobian(Vobs,IM,gridder):\n",
    "    #Degrid\n",
    "    Vpred = gridder.do_DFT_Vpred(IM)\n",
    "    \n",
    "    #Get residual vis\n",
    "    Vres = Vobs - Vpred\n",
    "    \n",
    "    #Grid residuals to find IR\n",
    "    IR = np.real(gridder.give_IR(Vres))\n",
    "\n",
    "    return IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Doing CS CLEAN\"\n",
    "IM, IR = CLEAN_CS(Vobs.copy(),ID.copy(),PSF.copy(),gridder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ch = nchan/2\n",
    "F = plt.figure(1,(15,15))\n",
    "grid = ImageGrid(F, 111,  # similar to subplot(111)\n",
    "                nrows_ncols=(1, 2),\n",
    "                direction=\"row\",\n",
    "                axes_pad=0.5,\n",
    "                add_all=True,\n",
    "                label_mode=\"1\",\n",
    "                share_all=True,\n",
    "                cbar_location=\"right\",\n",
    "                cbar_mode=\"each\",\n",
    "                cbar_size=\"3%\")\n",
    "\n",
    "grid[0].set_title(\"Model\")\n",
    "im = grid[0].imshow(IM[ch], interpolation=\"nearest\", cmap=\"cubehelix\")\n",
    "grid[0].cax.colorbar(im)\n",
    "grid[1].set_title(\"Residual\")\n",
    "im = grid[1].imshow(IR[ch], interpolation=\"nearest\", cmap=\"cubehelix\")\n",
    "grid[1].cax.colorbar(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "I = np.argwhere(IM[ch] > 0.3)\n",
    "Ix = I[:,0]\n",
    "Iy = I[:,1]\n",
    "for i in range(Ix.size):\n",
    "    print r\"Source at (%s, %s) has flux of %s Jy at v = %s Hz\" % (Ix[i], Iy[i],IM[ch,Ix[i],Iy[i]],Freqs[ch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as there are two or more channels it is possible to fit a spectral index model to each source in the model image. This can be done by fitting a straight line to \n",
    "$$ \\ln I = \\ln I_0 + \\alpha \\ln (\\frac{\\nu}{\\nu_0}), $$\n",
    "or equivalently solving the linear system\n",
    "$$ \\left[ \\begin{array}{c} \\ln I(\\nu_0) \\\\ \\ln I(\\nu_1) \\\\ \\cdot \\\\ \\ln I(\\nu_{nv}) \\end{array} \\right] = \\left[ \\begin{array}{cc} 1 & \\ln \\frac{\\nu_0}{\\nu_{ref}} \\\\ 1 & \\ln \\frac{\\nu_1}{\\nu_{ref}} \\\\ \\cdot & \\cdot \\\\\n",
    "1 & \\ln \\frac{\\nu_{nv}}{\\nu_{ref}} \\end{array} \\right] \\left[ \\begin{array}{c} \\ln I(\\nu_{ref}) \\\\ \\alpha \\end{array} \\right] $$\n",
    "where $nv$ is the number of channels/bands. We can do it for all the sources simultaneously by solving\n",
    "$$ \\left[ \\begin{array}{cccc} \\ln I_{s_1}(\\nu_0) & \\ln I_{s_2}(\\nu_0) & \\cdot & \\cdot \\\\ \\ln I_{s_1}(\\nu_1) & \\ln I_{s_2}(\\nu_1) & \\cdot & \\cdot \\\\ \\cdot \\\\ \\ln I_{s_1}(\\nu_{nv}) & \\ln I_{s_2}(\\nu_{nv}) & \\cdot & \\cdot \\end{array} \\right] = \\left[ \\begin{array}{cc} 1 & \\ln \\frac{\\nu_0}{\\nu_{ref}} \\\\ 1 & \\ln \\frac{\\nu_1}{\\nu_{ref}} \\\\ \\cdot & \\cdot \\\\\n",
    "1 & \\ln \\frac{\\nu_{nv}}{\\nu_{ref}} \\end{array} \\right] \\left[ \\begin{array}{cccc} \\ln I_{s_1}(\\nu_{ref}) & \\ln I_{s_2}(\\nu_{ref}) & \\cdot & \\cdot \\\\ \\alpha_{s_1} & \\alpha_{s_2} & \\cdot & \\cdot \\end{array} \\right] $$\n",
    "The code below fits a spectral model to the sources above some hand picked threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_SPI(IM,Freqs,ref_freq,threshold):\n",
    "    nchan = Freqs.size\n",
    "    #Find all sources with flux above threshold (should probably check this in each frequency band)\n",
    "    I = np.argwhere(IM[nchan/2] > threshold)\n",
    "    Ix = I[:,0]\n",
    "    Iy = I[:,1]\n",
    "    #Get the model Image as a function of frequency at all these locations\n",
    "    nsource = Ix.size\n",
    "    logIM = np.zeros([nchan,nsource])\n",
    "    for i in xrange(nsource):\n",
    "        logIM[:,i] = np.log(IM[:,Ix[i],Iy[i]])\n",
    "        \n",
    "    #Create the design matrix\n",
    "    X = np.ones([nchan,2])\n",
    "    X[:,1] = np.log(Freqs/ref_freq)\n",
    "    \n",
    "    #Solve the system\n",
    "    logIref, alpha = np.dot(np.linalg.inv(X.T.dot(X)),np.dot(X.T,logIM))\n",
    "    for i in xrange(nsource):\n",
    "        print r\"Source at (%s, %s) has flux of %s Jy at v = %s Hz and alpha = %s\" % (Ix[i],Iy[i],np.exp(logIref[i]),ref_freq,alpha[i])    \n",
    "    return np.exp(logIref), alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Iref, alpha = fit_SPI(IM,Freqs,ref_freq,0.4) #Note hard-coded threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Frequency-Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For MFS we start by expanding the model image $I^M(\\nu)$ into a power series in frequency eg. \n",
    "$$ I^M(\\nu) = \\sum_{t=0}^{N_t-1} w(\\nu)^t I^T_t, $$\n",
    "where $I^T_t$ is the $t^{th}$ power series coefficient and $w(\\nu)$ can be thought of as basis functions. Although this can be attempted for any power series the coeffcients only take on physical meaning when we have some a priori model for the frequency dependence of $I^M$. The most common such model is the spectral index model which postulates that \n",
    "$$ I^M(\\nu) = I^M(\\nu_0) \\left(\\frac{\\nu}{\\nu_0}\\right)^\\alpha, $$\n",
    "where $\\nu_0$ is some reference frequency. Performing a Taylor expansion about $\\nu_0$ shows that if we use $w(\\nu) = \\left(\\frac{\\nu - \\nu_0}{\\nu_0}\\right)$ as basis functions then the first few series coeficients can be related to $I^M_0 = I^M(\\nu_0)$ and $\\alpha$ according to  \n",
    "$$ I^T_0 = I^M_0, \\quad I^T_1 = \\alpha I^M_0, $$\n",
    "so that we get an alpha map from\n",
    "$$ \\alpha = \\frac{I^T_1}{I^T_0}. $$\n",
    "More complicated models can be used in a similar way. As we show below we can determine the coefficients $I^T$ corresponding to arbitrary weights $w(\\nu)$. A model is only required to determine what these coefficients mean.\n",
    "\n",
    "Substituting the series expansion into the expression for $V^{obs}$ gives\n",
    "$$ V^{obs}_\\nu = \\sum_{t = 0}^{N_t-1} w(\\nu)^t S_\\nu F I^T_t = \n",
    "\\left[ \\begin{array}{cccc}\n",
    "[ w^0_v S_v F] & [ w^1_v S_v F] & \\cdot & \\cdot\n",
    "\\end{array} \\right] \\left[\\begin{array}{c} I^T_0 \\\\ I^T_1 \\\\ \\cdot \\\\ \\cdot \\end{array} \\right], $$\n",
    "where in the last step we perform the summation using block matrices. MFS basically works by using all the visibility data (i.e. in each channel/band) to estimate the coefficients $I^T$. This can be achieved by solving the following system\n",
    "$$\n",
    "\\left[ \\begin{array}{cccc}\n",
    "[ w^0_{v_1} S_{v_1} F] & [ w^1_{v_1} S_{v_1} F] & \\cdot & \\cdot \\\\\n",
    "[ w^0_{v_2} S_{v_2} F] & [ w^1_{v_2} S_{v_2} F] & \\cdot & \\cdot \\\\\n",
    "\\cdot & \\cdot & \\cdot & \\cdot\n",
    "\\end{array} \\right] \\left[\\begin{array}{c} I^T_0 \\\\ I^T_1 \\\\ \\cdot \\\\ \\cdot \\end{array} \\right] = \\left[\\begin{array}{c} V^{obs}_{\\nu_1} \\\\ V^{obs}_{\\nu_2} \\\\ \\cdot \\\\ \\cdot \\end{array} \\right].\n",
    "$$\n",
    "The normal equations for this system can be found simply by multiplying through by the conjugate transpose of the matrix on the left\n",
    "$$\n",
    "\\left[ \\begin{array}{ccc}\n",
    "[ w^0_{v_1} S^\\dagger_{v_1} F^\\dagger] & [w^0_{v_2} S^\\dagger_{v_2} F^\\dagger] & \\cdot \\\\\n",
    "[ w^1_{v_1} S^\\dagger_{v_1} F^\\dagger] & [ w^1_{v_2} S^\\dagger_{v_2} F^\\dagger] & \\cdot \\\\\n",
    "\\cdot & \\cdot & \\cdot  \\\\\n",
    "\\cdot & \\cdot & \\cdot \n",
    "\\end{array} \\right]\n",
    "\\left[ \\begin{array}{cccc}\n",
    "[ w^0_{v_1} S_{v_1} F] & [ w^1_{v_1} S_{v_1} F] & \\cdot & \\cdot \\\\\n",
    "[ w^0_{v_2} S_{v_2} F] & [ w^1_{v_2} S_{v_2} F] & \\cdot & \\cdot \\\\\n",
    "\\cdot & \\cdot & \\cdot & \\cdot\n",
    "\\end{array} \\right] \\left[\\begin{array}{c} I^T_0 \\\\ I^T_1 \\\\ \\cdot \\\\ \\cdot \\end{array} \\right] = \n",
    "\\left[ \\begin{array}{ccc}\n",
    "[ w^0_{v_1} S^\\dagger_{v_1} F^\\dagger] & [w^0_{v_2} S^\\dagger_{v_2} F^\\dagger] & \\cdot \\\\\n",
    "[ w^1_{v_1} S^\\dagger_{v_1} F^\\dagger] & [ w^1_{v_2} S^\\dagger_{v_2} F^\\dagger] & \\cdot \\\\\n",
    "\\cdot & \\cdot & \\cdot  \\\\\n",
    "\\cdot & \\cdot & \\cdot \n",
    "\\end{array} \\right]\n",
    "\\left[\\begin{array}{c} V^{obs}_{\\nu_1} \\\\ V^{obs}_{\\nu_2} \\\\ \\cdot \\\\ \\cdot \\end{array} \\right].\n",
    "$$\n",
    "or\n",
    "$$ H I^T = I^R, $$\n",
    "where the images $I^R$ on the RHS are constructed from $I^D_{\\nu}$ according to\n",
    "$$ I^R_i = \\sum_{\\nu_k} w_{\\nu_k}^i S^\\dagger_{\\nu_k} F^\\dagger V^{obs}_{\\nu_k} = \\sum_{\\nu_k} w_{\\nu_k}^i I^D_{\\nu_k}, $$\n",
    "and the Hessian matrix consists of block matrices of the following form\n",
    "$$ H_{ij} = \\sum_{\\nu_k} w_{\\nu_k}^{i+j} F^\\dagger S^\\dagger_{\\nu_k} S_{\\nu_k} F. $$\n",
    "Each Hessian block can be thought of as a spectral PSF (denoted $I^{PSF}_{i,j}$) and the solution to the normal equations returns the components $I^T$. The full Hessian matrix will not, in general, be invertible. However, by making the same assumptions as before, we can reduce each $H_{ij}$ to a single number viz. \n",
    "$$ \\tilde{H}_{ij} = \\sum_{\\nu_k} w_{\\nu_k}^{i + j} \\mbox{mid}(I^{PSF}_{\\nu_k}). $$\n",
    "The deconvolution can be approximated by searching for the peak in $I^R_0$ (since $I^R_0 = \\sum_{\\nu_k} I^D_{\\nu_k}$ this is similar to searching the average image over channels) and computing $I^T$ using\n",
    "$$ I^T = H^{-1}I^R $$\n",
    "The components of $I^T$ can be used to get $I^M$ so these components can be used to compute the predicted visibilities and hence update the residual image by evaluating the Jacobian in the major cycle. Subtraction within the image domain during the minor cycle is slightly trickier though. According to the original paper we have to update $I^R$ according to\n",
    "$$ I^R_i = I^R_i - g \\sum_{j = 0}^{N_t - 1} \\hat{I}^{PSF}_{i,j} * I^T_{j,(k)}, $$\n",
    "where $I^T_{j,(k)}$ is the j${}^{th}$ coefficient at iteration $k$ and $\\hat{I}^{PSF}_{i,j}$ is the spectral PSF normalised by $\\hat{I}^{PSF}_{0,0}$. Substituting in the definition $I^{PSF}_{ij} = \\sum_{\\nu_k} w_{\\nu_k}^{i+j} F^\\dagger S^\\dagger_{\\nu_k} S_{\\nu_k} F$ shows that the minor cycle subtraction step is equivalent to\n",
    "$$ I^R_{i,(k+1)} = I^R_{i,(k)} - g \\sum_{\\nu_k} w(\\nu_k)^i \\hat{I}^{PSF}_{\\nu_k} \\sum_{j = 0}^{N_t-1} w(\\nu_k)^j I^T_{j,(k)} = I^R_i - g \\sum_{\\nu_k} w(\\nu_k)^i \\hat{I}^{PSF}_{\\nu_k} I^M_{(k)}(\\nu_k) $$\n",
    "where we have used $I^M(\\nu_k) = \\sum_{j=0}^{N_t-1} w(\\nu_k)^j I^T_j$. This should be compared with the definition of $I^R_i = \\sum_{\\nu_k} w_{\\nu_k}^i I^D_{\\nu_k}$. A crude implementation of this algorithm is given below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def CLEAN_MFS(Vobs,ID,PSF,Nt,Freqs,ref_freq,gridder,\n",
    "             gamma = 0.05,threshold = \"Default\", nminoriter = 20,\n",
    "             nmajoriter = 100):\n",
    "    \"\"\"\n",
    "    This is a basic MFS CLEAN implemented using major and minor cycles. \n",
    "    Input:  Vobs = precalibrated visibilities\n",
    "            lmn = the coordinates of the image\n",
    "            K = the DFT kernel\n",
    "            invK = the inverse DFT kernel\n",
    "            PSF = the unnormalised PSF\n",
    "    \"\"\"\n",
    "    #Initialise model image\n",
    "    n = Vobs.shape[0]\n",
    "    nchan = Vobs.shape[1]\n",
    "    npix = ID.shape[2]\n",
    "    IT = np.zeros([Nt,npix,npix])\n",
    "    Istar = np.zeros(Nt)\n",
    "    \n",
    "    w = (Freqs - ref_freq)/ref_freq\n",
    "    \n",
    "    #Normalise PSF and ID keeping track of maximum\n",
    "    PSF_max = np.zeros(nchan)\n",
    "    for nu in xrange(nchan):\n",
    "        PSF_max[nu] = np.max(PSF[nu])\n",
    "        #PSF[i] /= PSF_max[0]\n",
    "        #ID[i] /= PSF_max[0]\n",
    "\n",
    "    #Compute weighted sum of dirty's\n",
    "    print \"Getting I from ID\"\n",
    "    IR = set_Dirty_Comps(ID,w,Nt)\n",
    "    \n",
    "    Itmp = IR[0] #this is where we do the peak finding\n",
    "    \n",
    "    #Find first maximum\n",
    "    p,q = (argwhere(Itmp == Itmp.max())[0]).squeeze()\n",
    "    Istarm = Itmp[p,q]\n",
    "    \n",
    "    #Set the Hessian and get inverse\n",
    "    print \"Setting H\"\n",
    "    invH, Ipsf = set_Hessian(PSF,PSF_max,w,Nt,nchan,npix)\n",
    "    \n",
    "    #Get principle solution (Taylor coefficients)\n",
    "    Istar = np.dot(invH,IR[:,p,q])\n",
    "\n",
    "    #Set threshold\n",
    "    if threshold==\"Default\":\n",
    "        threshold = 0.25*np.abs(Istarm) \n",
    "        print \"Threshold set at \", threshold\n",
    "    else:\n",
    "        print \"Assuming user set threshold\"\n",
    "        \n",
    "    #Start deconvolution\n",
    "    i = 0\n",
    "    print \"Deconvolving\"\n",
    "    while (i < nmajoriter) and (Istarm > 0.5*threshold):\n",
    "        #Enter minor cycle\n",
    "        j = 0\n",
    "        while (j < nminoriter) and (Istarm > threshold):\n",
    "            #print \"j = \", j, \"Istarm = \", Istarm, \"Istar = \", Istar\n",
    "            #Update model image\n",
    "            IT[:,p,q] += gamma*Istar\n",
    "            \n",
    "            #Do image plane subtraction\n",
    "            for t in xrange(Nt):\n",
    "                tmp = 0.0\n",
    "                for k in range(Nt):\n",
    "                    tmp += gamma*Istar[k]*Ipsf[t,k,npix - p:2*npix - p,npix - q:2*npix - q]\n",
    "                IR[t] -= tmp\n",
    "            \n",
    "            #Get new I to search in\n",
    "            #I = set_Dirty_Comps(IR,w,Nt)\n",
    "            Itmp = IR[0]\n",
    "            \n",
    "            #Get new indices where I is max\n",
    "            p,q = (argwhere(Itmp == Itmp.max())[0]).squeeze()\n",
    "            Istarm = Itmp[p,q]\n",
    "            \n",
    "            #Solve of IT\n",
    "            Istar = np.dot(invH,IR[:,p,q])\n",
    "            \n",
    "            #Increment minor cycle counter\n",
    "            j += 1         \n",
    "        #Increment major cycle counter\n",
    "        i += 1\n",
    "            \n",
    "        #Update IT\n",
    "        IT[:,p,q] += gamma*Istar\n",
    "        \n",
    "        #Get residual image\n",
    "        IR = get_Jacobian_MFS(Vobs,IT,npix,PSF_max,gridder,w,Nt)\n",
    "        \n",
    "        #Find next peak\n",
    "        Itmp = IR[0]\n",
    "        p,q = (argwhere(Itmp == Itmp.max())[0]).squeeze()\n",
    "        Istarm = Itmp[p,q]\n",
    "        \n",
    "        #get principal solution\n",
    "        Istar = np.dot(invH,IR[:,p,q])\n",
    "        \n",
    "        if i%5==0:\n",
    "            print \"i = \", i, \"j = \", j, \"Istarm = \", Istarm\n",
    "    \n",
    "            \n",
    "    #Warn if number of iterations exceeded\n",
    "    if i >= nmajoriter:\n",
    "        print \"Max iterations exceeded. Istarm = \",Istarm\n",
    "    \n",
    "    print \"Done\"\n",
    "    \n",
    "    #Get IM\n",
    "    IM = np.zeros([nchan,npix,npix])\n",
    "    for nu in xrange(nchan):\n",
    "        for t in xrange(Nt):\n",
    "            IM[nu] += w[nu]**t*IT[t] \n",
    "    \n",
    "    #Get alpha map (we only compute this for pixels above a certain threshold)\n",
    "    \n",
    "    I = np.argwhere(IM[nchan/2] > threshold) #.squeeze()\n",
    "    Ix = I[:,0]\n",
    "    Iy = I[:,1]\n",
    "    alpha = np.zeros([npix,npix])\n",
    "    alpha[Ix,Iy] = IT[1,Ix,Iy]/IT[0,Ix,Iy]\n",
    "    return IM, IT, IR , alpha\n",
    "\n",
    "def get_Jacobian_MFS(Vobs,IT,npix,PSF_max,gridder,w,Nt):\n",
    "    #Get number of channels/bands\n",
    "    n = Vobs.shape[0]\n",
    "    nchan = Vobs.shape[1]\n",
    "    npix = IT.shape[-1]\n",
    "\n",
    "    IM = np.zeros([nchan,npix,npix])\n",
    "    for k in xrange(nchan):\n",
    "        for l in xrange(Nt):\n",
    "            IM[k] += w[k]**l*IT[l]            \n",
    "    \n",
    "    Vpred = gridder.do_DFT_Vpred(IM)\n",
    "    \n",
    "    #Get residual vis\n",
    "    Vres = Vobs - Vpred\n",
    "    \n",
    "    #Get residual image\n",
    "    ID = np.real(gridder.give_IR(Vres))\n",
    "    \n",
    "    #Get I\n",
    "    IR = set_Dirty_Comps(ID,w,Nt)\n",
    "    return IR\n",
    "\n",
    "def set_Hessian(PSF,PSF_max,w,Nt,nchan,npix):\n",
    "    \"\"\"\n",
    "    Constructs principal components of the Hessian\n",
    "    \"\"\"\n",
    "    #Create array to store Hessian\n",
    "    H = np.zeros([Nt,Nt])\n",
    "    Ipsf = np.zeros([Nt,Nt,2*npix,2*npix])\n",
    "    #Compute Hessian\n",
    "    for i in xrange(Nt):\n",
    "        for j in xrange(Nt):\n",
    "            for k in range(nchan): \n",
    "                H[i,j] += w[k]**(i+j)*PSF_max[k]\n",
    "                Ipsf[i,j] += w[k]**(i+j)*PSF[k]\n",
    "    return np.linalg.inv(H), Ipsf/Ipsf[0,0].max()\n",
    "\n",
    "def set_Dirty_Comps(ID,w,Nt):\n",
    "    nchan,nx,ny = ID.shape\n",
    "    IR = np.zeros([Nt,nx,ny])\n",
    "    for t in xrange(Nt):\n",
    "        for i in xrange(nchan):\n",
    "            IR[t] += w[i]**t*ID[i]\n",
    "    return IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Nt = 3\n",
    "IM, IT, IR, alpha = CLEAN_MFS(Vobs.copy(),ID.copy(),PSF.copy(),Nt,Freqs,ref_freq,gridder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ch = 0\n",
    "F = plt.figure(1,(15,15))\n",
    "grid = ImageGrid(F, 111,  # similar to subplot(111)\n",
    "                nrows_ncols=(1, 2),\n",
    "                direction=\"row\",\n",
    "                axes_pad=0.5,\n",
    "                add_all=True,\n",
    "                label_mode=\"1\",\n",
    "                share_all=True,\n",
    "                cbar_location=\"right\",\n",
    "                cbar_mode=\"each\",\n",
    "                cbar_size=\"3%\")\n",
    "\n",
    "grid[0].set_title(\"Model\")\n",
    "im = grid[0].imshow(IM[ch], interpolation=\"nearest\", cmap=\"cubehelix\")\n",
    "grid[0].cax.colorbar(im)\n",
    "grid[1].set_title(\"Residual\")\n",
    "im = grid[1].imshow(IR[ch], interpolation=\"nearest\", cmap=\"cubehelix\")\n",
    "grid[1].cax.colorbar(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#I = np.argwhere(IM[ch] > 0.25)\n",
    "I = np.argwhere(abs(alpha) > 0.1)\n",
    "Ix = I[:,0]\n",
    "Iy = I[:,1]\n",
    "for i in range(Ix.size):\n",
    "    #print r\"Source at (%s, %s) has flux of %s Jy at v = %s Hz\" % (Ix[i], Iy[i],IM[ch,Ix[i],Iy[i]],Freqs[ch])\n",
    "    print r\"Source at (%s, %s) has alpha = %s Jy\" % (Ix[i], Iy[i],alpha[Ix[i],Iy[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
